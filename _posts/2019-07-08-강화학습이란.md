---
layout: post
title: "pyTorch를 활용한 강화학습 실전입문 - 1. 강화학습이란 무엇인가?"
tags: [강화학습,RL,pytorch]
use_math: true
comments: true
---
### Pytorch를 활용한 강화학습/심층강화학습 실전 입문

이번에 대전에서 2번째로 열린 DLCAT(2nd 딥러닝 컨퍼런스)에 참여했다. 10시부터 시작해서 5시 반까지 이어진 섹션에서 다양한 강의를 듣고 열심히해야겠다고 생각을 많이했다.

그 때 설문조사를 열심히 작성해준 사람 50명에게 준다고 그러길래 혹시나 하는 마음에 끄적여봤는데 운좋게도 나도 50명 중 한명이 되어서 공짜 책을 받게되었다 ㅎㅎㅎ.

기왕 받은거 열심히 하자는 뜻으로 책을 읽고 한번 정리해보자고 생각이 들어서 블로그에 포스팅으로 남기기로 했다.

### 강화학습

#### 강화학습이란?

강화학습은 **Rewards**(보상)을 최대화하는 **action**(행동)을 하기위해 스스로 학습하는 것이다. 강화학습에는 action을 하는 **agent**와 agent가 action을 하기위해 필요한 정보(state, rewards)들을 주는 **environment**로 구성되어 있다.

강화학습의 기본구조는 다음과 같다. agent가 environment에게 받은 **state(상태)**를 통해서 action을 취하게 된다. 그러면 environment는 다시 agent의 action을 받아서 그에 상응하는 rewards과 행동으로 인한 새로운 state를 agent에게 준다. 그러면 agent는 이러한 구조 안에서 보상이 최대가 되도록 행동을 하게 학습한다.

강화학습이 주목받는 이유는 강화학습이 뇌의 학습 메커니즘과 비슷하다는 점이다.

태어난지 얼마 안된 아기는 사람의 말을 이해하지 못한다. 하지만 스스로 보고, 듣고, 느끼는 환경 속에서 아기는 그 말을 이해하는 행동을 하므로서 그 말을 결국 이해하게 된다.

원인(action)에 대한 결과(rewards)가 최고가 되도록 학습하는 이러한 방식은 아기가 처한 state에 대한 어떤 action을 통해서 말을 이해하는 rewards를 받는 구조라고 생각하면 강화학습이 인간의 뇌 매커니즘과 비슷하다고 할 수 있는 것이다.

##### 강화학습의 한계

강화학습이 최초로 제시된건 1990년대 후반이다. 1990년대 후반부터 2000년대 초에 이르기까지 강화학습 연구가 엄청나게 활성화 되었지만 크게 성과를 이뤄내지 못했다. 그 이유는 **많은 정보들을 전부 담아낼 수 없었기 때문**이다.

아까전에 강화학습의 기본 구조에 대해서 설명할 때 state를 받아서 action으로 반환하다고 했는데, 하나의 태스크에서 이 구조를 통해 학습하기 위해서는 (state의 패턴 수)x(action의 수)가 된다. 즉 상태의 패턴이 많고 행동의 수가 많다면 그 정보를 모두 사용하기에는 너무나도 많다. 그래서 나온 새로운 방법은 state의 정보를 모두 취하는 것이 아니라 필요한 정보만 추려서 나타내는 방법이었다. 간단한 예시로 장기에서 장기판 위에 있는 모든 state를 다루는 대신 실제 장기판 위에 있는 **돌들의 위치 관계**만 따지도록하는 것이다. 하지만 현실에서는 이러한 요약된 정보를 추출하는 방법이 명확하지 않기 때문에 여전히 남아있는 문제였다.

##### 딥러닝의 결합

이러한 state를 다루는데의 문제는 딥러닝의 도입으로 해결하게 되었다. 딥러닝은 3층(입력층, 은닉층, 출력층)으로 구성된 신경망에서 중간층을 계속 깊게 만든 것을 말한다. 딥러닝은 고차 방정식을 푸는데 능하기 때문에 이를 활용해서 강화학습을 사용하는게 가능해졌다. 이렇게 강화학습에 딥러닝을 접목한 기법을 심층강화학습이라고 한다. 심층강화학습은 DQN(Deep Q-Network)라는 알고리즘으로 유명해졌다.